import java.io.*;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;

/**
 * An InstanceSet is a set of instances to be used in a machine learning
 * problem, generally as either a training set or a test set.
 * 
 * @author jmac
 * 
 */
public class InstanceSet {
	// All instances in the InstanceSet share the same set of
	// attributes, stored here as an AttributeSet
	private AttributeSet attributeSet = new AttributeSet();

	// An ArrayList containing all instances in the InstanceSet
	private ArrayList<Instance> instances = new ArrayList<Instance>();

	// List of all of the numeric data
	private HashMap<Integer, ArrayList<Double>> numericData = new HashMap<Integer, ArrayList<Double>>();
	// list of indexes of data on line
	private ArrayList<Integer> numericIndexes = new ArrayList<Integer>();

	public static final boolean VERBOSE = false;

	public static final int NUM_BUCKETS = 15;

	
	/**
	 * 
	 */
	private String[] mostCommonDataValues;

	/**
	 * 
	 */
	private ArrayList<HashMap<String, Integer>> mostCommonDataValuesCounter = new ArrayList<HashMap<String, Integer>>();
	
	
	/**
	 * The character used to start comments in .arff files.
	 */
	public static final String commentStart = "%";

	/**
	 * Construct an InstanceSet by reading a .arff file with the given filename
	 * 
	 * @param inputFilename name of the file to read
	 * @throws IOException
	 * @throws DecisionTreeException
	 * @throws IOException
	 * @throws Exception
	 */
	public InstanceSet(String inputFilename) throws DecisionTreeException, IOException {
		parseInputFile(inputFilename);
	}

	/**
	 * Construct an InstanceSet from a list of instances
	 * 
	 * @param attributeSet the set of attributes for this set of instances
	 * @param instances    a list of the instances to be stored in the instance set
	 */
	public InstanceSet(AttributeSet attributeSet, ArrayList<Instance> instances) {
		super();
		this.attributeSet = attributeSet;
		this.instances = instances;
	}

	public void printNumbericDataRanges() {
		for (int i : numericIndexes) {
			ArrayList<Double> arr = numericData.get(i);
			double range = arr.get(arr.size() - 1) - arr.get(0);
			System.out.println(
					"THE RANGE OF FIRST ATTRIBUTE: " + attributeSet.getAttributes().get(i).getName() + " is " + range);
		}
	}

	private void parseInputFile(String inputFilename) throws DecisionTreeException, IOException {
		BufferedReader reader = new BufferedReader(new FileReader(inputFilename));

		// Input file comprises a "preamble" followed by "data".
		// Data segment is demarcated by a line consisting of the string
		// "@data".

		// We make everything case insensitive by transforming to
		// lowercase before doing anything else.

		// We parse the preamble first, constructing attributes and
		// the like.
		String line = reader.readLine().toLowerCase();
		line = line.toLowerCase();
		while (!parsePreambleLine(line)) {
			line = reader.readLine();
			line = line.toLowerCase();
		}
		attributeSet.setDefaultClassAttribute();

		if (VERBOSE) {
			System.out.println("\nFinished processing preamble, attribute set is:");
			this.attributeSet.print();
		}

		// Now parse the data
		
		
		line = reader.readLine().toLowerCase();
		while (line != null) {
			line = line.toLowerCase();
			parseDataLine(line);
			line = reader.readLine();
		}
		reader.close();

		for (int i : numericIndexes) {
			if (VERBOSE)
				System.out.println("INDEX: " + i);
			Collections.sort(numericData.get(i));
		}

		// TO DO: MAKE NUMERIC INDEX LIST AND
		//// Replace index with bucket name
		for (int i : numericIndexes) {
			// decide on buckets
			for (Instance instance : instances) {
				Double value = Double.valueOf(instance.getValues()[i]);
				String bucketName = assignBucket(value, i);
				// need to write this assign bucket function
				instance.getValues()[i] = bucketName;
			}
		}

		if (VERBOSE) {
			System.out.println("\nFinished processing data, " + instances.size() + " instances found:");
			for (Instance instance : instances)
				instance.print();
			System.out.println();
		}

	}

	// Used in parse input file method
	// Assigns a bucket to a value
	// Makes continuous data discrete
	// from list of values decide what value bucket belongs to....
	private String assignBucket(double val, int numericIndex) {
		// TO DO: Fill in this method....
		ArrayList<Double> arr = numericData.get(numericIndex);
		int i = (arr.size()) / NUM_BUCKETS;
		for (int j = 1; j < NUM_BUCKETS; j++) {
			if (val < arr.get(i * j)) {
				return String.valueOf(j);
			}
		}
		return String.valueOf(NUM_BUCKETS);
	}

//	if (val < arr.get(i)) {
//	return "low";
//} else if (val < arr.get(i * 2)) {
//	return "medium-low";
//} else if (val < arr.get(i * 3)) {
//	return "medium";
//} else if (val < arr.get(i * 4)) {
//	return "medium-high";
//} else {
//	return "high";
//}

	private boolean shouldIgnoreLine(String line) {
		if (line.equals(""))
			return true;
		else if (line.startsWith(commentStart))
			return true;
		else if (line.startsWith("@relation"))
			return true;
		else if (line.contains("?"))
			return true;
		else
			return false;
	}

	// Return true if this line is the start of the data segment,
	// otherwise return false
	private boolean parsePreambleLine(String line) throws DecisionTreeException {
		if (shouldIgnoreLine(line))
			return false;
		else if (line.startsWith("@attribute")) {
			parseAttribute(line);
			return false;
		} else if (line.startsWith("@data"))
			return true;
		else
			throw new DecisionTreeException("unexpected line: " + line);
	}

	private void parseAttribute(String line) {
		// split on whitespace to extract attribute name

		String[] split_line = line.split("\\s+");
		String attribute_name = split_line[1];
		// Extract the part in braces, which is the list of possible
		// attribute values.
		// To keep things simple, we will split on all brace
		// characters, thus assuming that the only place either
		// brace character appears is at the start and end of the
		// attribute value list
		if (VERBOSE)
			System.out.println("ATTRIBUTE NAME: " + attribute_name);

		// Construct the attribute and add it to the attribute set
		if (!line.contains("continuous")) {

			split_line = line.split("[{}]");
			String value_list = split_line[1];

			// To get the actual values out of value_list, split on commas
			// and whitespace
			String[] values = value_list.split("[,\\s]+");
			Attribute attribute = new Attribute(attribute_name, values);
			this.attributeSet.addAttribute(attribute);
		} else {
			String[] values = new String[NUM_BUCKETS];
			for (int i = 0; i < NUM_BUCKETS; i++) {
				values[i] = String.valueOf(i + 1);
			}

			Attribute attribute = new Attribute(attribute_name, values);
			this.attributeSet.addAttribute(attribute);
			if (VERBOSE)
				System.out.print("ADDING TO NUMERIC INDEXES");
			numericIndexes.add(attributeSet.getAttributeIndex(attribute));

		}

	}

	private void parseDataLine(String line) {
		if (shouldIgnoreLine(line))
			return;

		// Line of data values should be comma-separated, so split on
		// commas and whitespace
		String[] values = line.split("[,\\s]+");

		// TO DO: Address my value and numeric indexes
		for (int index : numericIndexes) {
			double myval = Double.parseDouble(values[index]);
			if (numericData.get(index) != null) {
				numericData.get(index).add(myval);
			} else {
				ArrayList<Double> cur = new ArrayList<Double>();
				cur.add(myval);
				numericData.put(index, cur);

			}
		}

		Instance instance = new Instance(values);
		this.instances.add(instance);
	}

	/**
	 * Get the set of attributes used by all the instances in this instance set.
	 * 
	 * @return the attributeSet
	 */
	public AttributeSet getAttributeSet() {
		if (VERBOSE) {
			for (Attribute at : attributeSet.getAttributes()) {
				System.out.println("ATTRIBUTE NAME: " + at.getName());
				for (int i = 0; i < at.getValues().length; i++) {
					System.out.print(at.getValues()[i] + " ");
				}
				System.out.println();
			}
		}

		return attributeSet;
	}

	/**
	 * Get a list of all instances in this instance set.
	 * 
	 * @return the instances
	 */
	public ArrayList<Instance> getInstances() {
		return instances;
	}

	/**
	 * Get the number of instances in this instance set.
	 * 
	 * @return the number of instances in this set of instances
	 */
	public int getNumInstances() {
		return instances.size();
	}
}











///////////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////////
///////DECISION TREE CLASS COPY VS TESTS AND EXTRA STUFF///////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////////



import java.io.FileNotFoundException;
import java.io.IOException;
import java.util.ArrayList;

/**
 * A DecisionTree object represents a decision tree, as described in, for
 * example, the book "Artificial Intelligence" by Russell and Norvig (3rd
 * edition). A DecisionTree is constructed using a set of training examples, and
 * is capable of deciding the class of a novel example. Because decision trees
 * are recursive data structures, any given DecisionTree object could be a node
 * in a larger decision tree, referred to as the <i>full decision tree</i> in
 * the documentation below.
 * 
 * @author jmac
 */
public abstract class DecisionTree {

	/**
	 * The label assigned to the root node of a decision tree.
	 */
	public static final String ROOT_LABEL = "root";

	// The label on the edge leading to this DecisionTree node.
	// This corresponds to one of the possible values of the attribute on which
	// the parent node was split. (See figure 18.6 of Russell and Norvig for an
	// example.) Exception: the label on the root node of a DecisionTree is
	// the constant ROOT_LABEL.
	private String edgeLabel;

	// The depth of this object in the full decision tree, with the
	// root node having depth 0 by convention.
	protected int depth;

	public static final boolean VERBOSE = false;

	/**
	 * Construct a decision tree according to the recursive algorithm given in
	 * figure 18.5 of Russell and Norvig (third edition).
	 * 
	 * @param examples       The examples from which this tree should be learned.
	 * @param attributes     A list of attributes on which this tree is permitted to
	 *                       make decisions.
	 * @param parentExamples The examples from which the parent node of this
	 *                       DecisionTree object were learned. to construct the root
	 *                       node, <code>parentExamples</code> should be null.
	 * @param label          The label on the edge leading to this DecisionTree
	 *                       node, or <code>DecisionTree.ROOT_LABEL</code> for the
	 *                       root.
	 * @param depth          The depth of this node in the full decision tree.
	 * @return The constructed DecisionTree.
	 * @throws DecisionTreeException
	 */
	public static DecisionTree constructDecisionTree(InstanceSet examples, ArrayList<Attribute> attributes,
			InstanceSet parentExamples, String label, int depth) throws DecisionTreeException {
		// The algorithm closely mimics figure 18.5 of Russell and Norvig.
		if (examples.getNumInstances() == 0) {
			// TODO: fix the following line.
			// HINT: it should begin "return new ..."
			if (VERBOSE) {
				System.out.println("------------------------------ If Statement: 1 ----------------------------");
			}
			return new DecisionTreeLeaf(parentExamples, label, depth + 1);
		} else if (isPure(examples) || attributes.size() == 0) {
			// TODO: fix the following line.
			// HINT: it should begin "return new ..."
			if (VERBOSE) {
				System.out.println("------------------------------ If Statement: 2 ----------------------------");
			}
			return new DecisionTreeLeaf(examples, label, depth + 1);
		} else {
			// TODO: fix the following line.
			// HINT: it should begin "return new ..."
			if (VERBOSE) {
				System.out.println("------------------------------ If Statement: 3 ----------------------------");
			}
			return new DecisionTreeInternal(examples, attributes, label, depth + 1);

		}
	}

	// Return true if the given set of instances is pure, and false otherwise.
	private static boolean isPure(InstanceSet instances) {
		// TODO: fill in the body of this method and fix the return statement
		int index = instances.getAttributeSet().getClassAttributeIndex();
		String[] instanceVals = instances.getInstances().get(0).getValues(); // get class value of first instance
		String firstVal = instanceVals[0];
		for (int i = 1; i < instanceVals.length; i++) {
			if (!firstVal.equals(instanceVals[i])) {
				if (VERBOSE)
					System.out.println("THIS " + instanceVals[i] + " IS NOT A PURE INSTANCE " + firstVal);

				return false;
			}
		}
		return true;

	}

	/**
	 * Remove the attribute that defines an instance's classification, and return
	 * the result in a new list
	 * 
	 * @param attributeSet The original set of attributes from which the list
	 *                     <code>attributes</code> was drawn (this will be used to
	 *                     define which attribute is the class attribute -- the one
	 *                     to be removed).
	 * @param attributes   A list of attributes which is a subset of the attributes
	 *                     in <code>attributeSet</code>. This list will be left
	 *                     undisturbed.
	 * @return A new list, which is the same as <code>attributes</code>, but with
	 *         the classification attribute removed.
	 */
	@SuppressWarnings("unchecked")
	private static ArrayList<Attribute> removeClassAttribute(AttributeSet attributeSet,
			ArrayList<Attribute> attributes) {
		Attribute classAttribute = attributeSet.getClassAttribute();
		ArrayList<Attribute> newAttributes = (ArrayList<Attribute>) attributes.clone();
		newAttributes.remove(classAttribute);
		return newAttributes;
	}

	/**
	 * This protected constructor cannot be called by external code; decision trees
	 * should be constructed using the constructDecisionTree factory method.
	 * 
	 * @param label The label on the edge leading to this DecisionTree node, or
	 *              <code>DecisionTree.ROOT_LABEL</code> for the root.
	 * @param depth The depth of this node in the full decision tree.
	 */
	protected DecisionTree(String label, int depth) {
		this.edgeLabel = label;
		this.depth = depth;
	}

	/**
	 * Return the decision tree's decision for the given instance: that is, the
	 * classification that should be assigned to the instance.
	 * 
	 * @param attributes The set of attributes employed by the instance.
	 * @param instance   The instance to be classified.
	 * @return The classification of the given instance.
	 */
	public abstract String decide(AttributeSet attributes, Instance instance);

	/**
	 * Print out the DecisionTree in a human-readable form
	 */
	public void print() {
		// indent this node according to its depth in the full decision tree
		for (int i = 0; i < depth; i++) {
			System.out.print("    ");
		}
		System.out.print("---" + edgeLabel + "---");
	}

	/**
	 * Compute the error rate of the decision tree on the given test set.
	 * 
	 * @param testSet A set of examples on which the error rate will be computed.
	 * @return The error rate of the decision tree on the given test set.
	 */
	public double computeErrorRate(InstanceSet testSet) {
		int num_errors = 0;
		AttributeSet attributes = testSet.getAttributeSet();
		int classAttributeIndex = attributes.getClassAttributeIndex();
		for (Instance instance : testSet.getInstances()) {
			String decision = decide(attributes, instance);
			String classification = instance.getValues()[classAttributeIndex];
			if (!decision.equals(classification))
				num_errors++;
		}
		return (double) num_errors / testSet.getNumInstances();
	}

	/**
	 * Print out the decision of this decision tree on every instance in the given
	 * test set.
	 * 
	 * @param testSet The set of instances whose decisions will be printed.
	 */
	public void printDecisions(InstanceSet testSet) {
		AttributeSet attributes = testSet.getAttributeSet();
		for (Instance instance : testSet.getInstances()) {
			String decision = decide(attributes, instance);
			System.out.print("instance: ");
			instance.print();
			System.out.println("decision: " + decision);
			System.out.println();
		}
	}

	/**
	 * Constructs a decision tree from the data in a .arff file, prints out the
	 * tree, the error rate on the training set, and the decisions on each instance
	 * in the training set.
	 * 
	 * @param arguments Requires a single command line argument, what should be the
	 *                  name of a data file in .arff format.
	 * @throws DecisionTreeException
	 * @throws FileNotFoundException
	 * @throws IOException
	 */
	public static void main(String[] arguments) throws DecisionTreeException, FileNotFoundException, IOException {
//		if (arguments.length != 1) {
//			System.out.println("usage: java DecisionTree datafilename");
//			System.exit(0);
//		}

		// read the training set from disk
		// String inputFilename = arguments[0];
		String inputFilename = "data/soybean.train.arff";
		InstanceSet trainingSet = new InstanceSet(inputFilename);

		// Prints the range of all the attributes in the training set
//		if (VERBOSE) {
//			System.out.println();
//			trainingSet.printNumbericDataRanges();
//			System.out.println();
//		}

		// testing the instance set is valid
//		if (VERBOSE) {
//			for (Instance i : trainingSet.getInstances()) {
//				System.out.println("THE VALUES ARE: ");
//				for (int j = 0; j < i.getValues().length; j++) {
//					System.out.print(i.getValues()[j]);
//				}
//				System.out.println();
//
//			}
//		}

		// Construct the list of attributes that will be used by the decision
		// tree, but make sure to remove the class attribute, which obviously
		// should not be used for classification!
		AttributeSet attributetrainingSet = trainingSet.getAttributeSet();

		// testing attribute set is good
//		if (VERBOSE) {
//			for (Attribute at : attributeSet.getAttributes()) {
//				System.out.println("MAIN METHOD ATTIBURESET VALUES ARE: ");
//				for (int j = 0; j < at.getValues().length; j++) {
//					System.out.print(at.getValues()[j] + " ");
//				}
//				System.out.println();
//
//			}
//		}
		
		//Print Frequencies of the Attributes
//		System.out.println("THE TRAINING SET PROBABILITIES: ");
//		for(Attribute at: attributetrainingSet.getAttributes()) {
//			Distribution d = new Distribution(at);
//			d.printFrequencies();
//			d.printProbabilities();
//		}

		ArrayList<Attribute> attributes = attributetrainingSet.getAttributes();
		attributes = removeClassAttribute(attributetrainingSet, attributes);

		// Construct the decision tree itself
		DecisionTree decisionTree = DecisionTree.constructDecisionTree(trainingSet, attributes, null,
				DecisionTree.ROOT_LABEL, 0);

		String testFile = "data/soybean.test.arff";
		InstanceSet testSet = new InstanceSet(testFile);

		
		//Print Frequencies of the Attributes
//		System.out.println("THE TESTING SET PROBABILITIES: ");
//		for(Attribute at: testSet.getAttributeSet().getAttributes()) {
//			Distribution d = new Distribution(at);
//			d.printFrequencies();
//			d.printProbabilities();
//		}

		// Print the Range of the Attributes in the testFile
		
		// System.out.println();
		// testSet.printNumbericDataRanges();
		
		// print the tree, its error rate on the training set, and its decisions
		// on the training set
		//decisionTree.print();
		
		double training_error_rate = decisionTree.computeErrorRate(trainingSet);
		double testing_error_rate = decisionTree.computeErrorRate(testSet);

		// ERROR Rate of the decision tree
	//	System.out.println();
		System.out.println("Error rate on training set: " + training_error_rate);
		System.out.println("Error rate on testing set: " + testing_error_rate);
		System.out.println();

		// Decisions
		// decisionTree.printDecisions(testSet);
	
	}

}




